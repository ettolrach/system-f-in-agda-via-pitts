\subsection{The lambda calculus.}
The $\lambda$-calculus (pronounced \textit{lambda calculus}), is a theoretical model of computation
developed by Alonzo \citet{church_set_1932} and is the basis for System F. It
looks and works similarly to familiar functional programming languages, yet its definition is as
minimal as possible while still being Turing-complete. Although Turing machines would be invented
after the $\lambda$-calculus \citep{turing_computable_1937}, `turing-complete' has become a
shorthand for `universal method of computation'. Such a universal method was not Church's initial
goal, but is why we're still interested in the $\lambda$-calculus.

The $\lambda$-calculus includes three reductions. $\alpha$-conversion is the renaming of variables
such that the semantics of the program are not changed; terms which are semantically identical but
use different variable names are called $\alpha$-equivalent, and as the name would imply, this is an
equivalence relation \citep{pierce_types_2002}.

$\beta$-reduction describes function application. The evaluation rules for the $\lambda$-calculus
are as follows \citep{wadler_programming_2022} (where $N[M/x]$ means replacing the term $M$ for any occurrence of $x$ in the term $N$).

\begin{equation}
\label{equation:untyped_beta_rules}
\begin{gathered}
  \inferrule{ }{(\lambda \, x. N)M \mapsto_{\beta} N[M/x]} \; (\beta) \quad
  \inferrule{L \mapsto_{\beta} L'}{LM \mapsto_{\beta} L'M} \; (\xi_1) \quad\\
  \inferrule{M \mapsto_{\beta} M'}{LM \mapsto_{\beta} LM'} \; (\xi_2) \quad
  \inferrule
    {N \mapsto_{\beta} N'}
    {(\lambda \, x. N) \mapsto_{\beta} (\lambda \, x. N')} \; (\zeta)
\end{gathered}
\end{equation}

The last reduction is $\eta$-reduction; for all $\lambda$-abstractions $\lambda \, x. L x$, we have
that $\lambda \, x. f x \mapsto_{\eta} f$.

If no $\beta$-reductions can be performed on a term, we say that it is in its \textit{normal form}.
We shall represent an arbitrary number of successive $\beta$-reductions using the Kleene star
$\mapsto_{\beta^{\star}}$. The Church-Rosser theorem states that for any term $L$, if it
$\beta$-reduces to two terms $M$ and $N$, then there exists a common term $L'$ which both $M$ and
$N$ eventually $\beta$-reduce to \citep{church_properties_1936}. This will be an important
consideration when choosing an evaluation strategy (see section
\ref{background:evaluation_strategy}.)

Notably, some terms may not have a normal form. Take \textit{little omega} $\omega \triangleq
\lambda \, x. (x x)$ and \textit{omega} (sometimes called the \textit{omega combinator}) $\Omega \,
\triangleq \, \omega \omega$. After a $\beta$ reduction step, $\Omega$ will remain unchanged and we
can perform the same $\beta$ reduction.

\begin{equation*}
  (\lambda \, x. (x x)) (\lambda \, x. (x x)) \quad
  \mapsto_{\beta} \quad (\lambda \, x. (x x)) (\lambda \, x. (x x))
\end{equation*}

The \textit{y-combinator} $\mathcal{Y} \triangleq (\lambda \, f. (\lambda \, x. f (x x )) (\lambda
\, x. f (xx)))$ allows for recursion. It will $\beta$-reduce to the argument applied to itself,
$\mathcal{Y} L \mapsto_{\beta^{\star}} L (\mathcal{Y} L)$.

We say that these terms \textit{do not have a normal form}.

\subsection{The simply-typed lambda-calculus}
The simply-typed $\lambda$-calculus (STLC, sometimes given the symbol $\lambda^{\rightarrow}$) is an
extension of the untyped $\lambda$-calculus developed by Alonzo \citet{church_formulation_1940}. It
requires each term to have a \textit{type}. In the STLC, terms which do not have a normal form
cannot be given a type, thus, all expressions will eventually reduce to an irreducible
expression---their normal form. This is also called \textit{strong normalisation}
\citep{pierce_types_2002}. However, this means we can longer represent all of the terms of the
untyped $\lambda$-calculus (such as $\Omega$), thus, Turing-completeness is lost.

In the STLC, some base types need to be chosen. These are indeterminates which are not given
definitions. \citet{church_formulation_1940} originally used the symbols $\iota$ and $\sigma$ for
base types. For the below examples, I will also use $\iota$, but the choice of symbol
is arbitrary. Without any base types, our computational model becomes \textit{degenerate} (that is,
there are no terms) \citep{pierce_types_2002}. As well as the base type, we also have a function (or
arrow) type $\to$. For some types $A$ and $B$, we say that a term $L$ has type $A \to B$ if $L$ can be applied using a term of type $A$ and gives a result of type $B$.

We will also need a \textit{type context} (also called \textit{type environment}), which will
usually be given the symbol $\Gamma$ or $\Delta$. The purpose of a context is to keep track of
the types of free variables. A context is a partial map from variables to types $\Gamma \colon
V \to T$. Types are written next to terms using a colon. For example, if the context contained the
mapping $x \colon \iota$, then we could write a function which applies $x$ to its argument as follows
(notice how we separate the context from the term using the symbol $\vdash$);
\begin{equation*}
  x \colon \iota \in \Gamma \vdash (\lambda \, f \colon \iota \to \iota . f x) \colon \iota.
\end{equation*}

Sometimes we may choose to omit the type of the bound variable for clarity, so we could write
$(\lambda \, f. \lambda \, x. fx) \colon \iota$.

To give a type to an expression, we will use a set of \textit{typing rules}. If a term $L$ with a
context $\Gamma$ can be given a type $A$, we write $\Gamma \vdash L \colon A$ and call this a
\textit{type judgement}. As the focus of this work is on System F, we will omit the typing rules for
the STLC and instead present the typing rules for System F only.

In the STLC, we can't give a type to little omega, since we can't give a type to both the argument
and the argument applied to itself ($(\lambda \, x  \colon ? . x x) \colon ??$).

\subsection{System F.}
System F has been the formal background to what many modern programming
languages call \textit{generics}. For an anachronistic example, in Rust, we could write a function
which applies a function twice to an argument.

\begin{minted}[samepage]{rust}
fn twice<T>(f: impl Fn(T) -> T, x: T) -> T
{
    f(f(x))
}
\end{minted}

Since we used a \textit{type parameter} in the function's type signature (here \texttt{T}), we can
use any appropriate function which has the type signature $\texttt{T} \to \texttt{T}$. One such
function is \texttt{u64::isqrt}, the (flooring) square root function. If this function was invoked
with \texttt{twice(u64::isqrt, 81)}, its output would be \texttt{3}. In this case, the compiler can
infer that the type for \texttt{T} should be \texttt{u64}, so we don't need to specify it
explicitly.

System F is the STLC equipped with \textit{polymorphic types}, another term for type parameters. It
was independently discovered by Jean-Yves \citet{girard_interpretation_1972} and John
\citet{goos_towards_1974}. We introduce a $\Lambda$-abstraction (also called \textit{type-abstraction}) which will introduce a bound type variable, and type application.

We can write this \textit{twice} function like so in System F:
\begin{equation*}
  (\Lambda \, T. \lambda \, f \colon T \to T . \lambda \, x \colon T . f (f x))
  \colon \forall T . (T \to T) \to T \to T.
\end{equation*}

Unlike the Rust example above, if we wanted to use the above \textit{twice} function, we would need
to explicitly perform a type application to specify what type we're using. The type inference
required for implicit type application is beyond the scope of this formalisation. We write type
applications using [square brackets]. For instance,
\begin{align*}
  s \colon \iota \to \iota, z \colon \iota \in \Gamma \vdash
    &(\Lambda \, T. \lambda \, f \colon T \to T . \lambda \, x \colon T . f (f x))
    [\iota] s z \colon \iota\\
  &\mapsto_{\beta}
    (\lambda \, f \colon \iota \to \iota . \lambda \, x \colon \iota . f (f x)) s z \colon \iota\\
  &\mapsto_{\beta^{\star}} s s z \colon \iota.
\end{align*}

The typing rules for System F will be given in section \ref{chapter3:type_judgements}.
